{"cells":[{"cell_type":"markdown","source":["# Create an asset hierarchy\nIngest an Asset hierarchy from raw to clean\n\nFill out Cmd 2 and run the whole Notebook. It will create an Asset Hierarchy in clean out of a table with Parent/Child relationships. All data in that table in RAW will be added as Metadata in CLEAN."],"metadata":{}},{"cell_type":"code","source":["secret_scope = \"your scope\" # name your secret scope here\nproject = \"project key\" # name the key to use from secret scope here\nraw_db = \"databasename\" #name of raw db your asset hierarchy is stored in\nraw_table = \"tablename\"  #name of raw table your asset hierarchy is stored in\ncolumn_parent = \"parent ID\" #column containing the parent key\ncolumn_child = \"ID\" #column containing the key\ncolumn_description = \"Description\" #column that should be displayed as Description in clean Assets\nroot_child = \"ROOT\" # value in 'column_child' column that the root node has \ntreename = \"Tree\" # Unique name of the tree you're ingesting "],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["assets_clean = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"assets\") \\\n    .option(\"apiKey\", dbutils.secrets.get(secret_scope, project)) \\\n    .option(\"inferSchema\", \"true\") \\\n    .load()\nassets_clean.createOrReplaceTempView(\"assets_clean\")\n\n\ninput = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"raw\") \\\n    .option(\"database\", raw_db) \\\n    .option(\"table\", raw_table) \\\n    .option(\"apiKey\", dbutils.secrets.get(secret_scope, project)) \\\n    .option(\"inferSchema\",\"true\") \\\n    .load()\ninput.createOrReplaceTempView(\"input\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["import pyspark.sql.functions as F\nmetadata_map_columns = [ item for f in input.schema for item in (F.lit(f.name), \"`\" + f.name + \"`\") ]\n#metadata_map_columns = [ item for f in before.schema if f.name not in asset_fields for item in (F.lit(f.name), \"`\" + f.name + \"`\")]\n\nlookup_metadata = input.select(F.col(column_child), F.create_map(*metadata_map_columns+[F.lit(\"treename\"),F.lit(treename)]).alias(\"metadata\"))\nlookup_metadata.createOrReplaceTempView(\"lookup_metadata\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\nmetadata_map_columns = [ \"input.\"+f.name for f in input.schema ]\n\ndf_base = spark.sql(\"select `\" + column_child + \"` as uniqueChild, cast(null as string) as uniqueParent,  cast(null as string) as parent,  `\" + column_child + \"` as child , 0 as depth from input where `\"+column_child+\"` = \\\"\"+root_child+\"\\\"\")\n#df_base = spark.sql(\"select `\" + column_child + \"` as uniqueChild, cast(null as string) as uniqueParent,  cast(null as string) as parent,  `\" + column_child + \"` as child , 0 as depth from input where `\"+column_parent+\"` is null\")\ndf_last_it = df_base\ndf_last_it.createOrReplaceTempView(\"t_last_it\")\n\ncnt = 0\ncnt_df_tmp_new= 1\n\ndf_write = spark.sql(\"SELECT child as id, null as path, null as element, t_last_it.child as name, null as parentId, `\" + column_description + \"` as description, lookup_metadata.metadata as metadata, null as source, null as sourceId, null as createdTime , null \\\n  as lastUpdatedTime from input, t_last_it, lookup_metadata where input.`\" + column_child + \"` == t_last_it.child AND lookup_metadata.`\" + column_child + \"` == t_last_it.child\")\ndf_write.printSchema()\ndf_write.write.insertInto(\"assets_clean\")\n\nrelationships = spark.sql(\"select `\" + column_parent + \"`  as parent, `\" + column_child + \"` as child from input\")\nrelationships.createOrReplaceTempView(\"relationships\")\n\nwhile cnt_df_tmp_new != 0 and cnt <= 20:\n  df_tmp_new = spark.sql(\"(select concat(relationships.child,concat('_',t_last_it.uniqueChild)) as uniqueChild, t_last_it.uniqueChild as uniqueParent, relationships.parent as parent, relationships.child as child  from t_last_it cross JOIN relationships ON t_last_it.child = relationships.parent )\")\n  cnt = cnt+1\n  cnt_df_tmp_new = df_tmp_new.count()\n  print(\"layer: {}\".format(cnt))\n  print(\"number of new Assets to be added this layer: {}\".format(cnt_df_tmp_new))\n  df_base = df_base.union(df_tmp_new.withColumn(\"depth\",F.lit(cnt)))\n  df_last_it = df_tmp_new\n  df_last_it.createOrReplaceTempView(\"t_last_it\")\n  df_write = spark.sql(\"SELECT child as id, null as path, null as element, t_last_it.child as name, assets_clean.id as parentId, `\" + column_description + \"` as description, lookup_metadata.metadata as metadata, null as source, null as sourceId, null as createdTime , null \\\n  as lastUpdatedTime from input, t_last_it, assets_clean, lookup_metadata where input.`\" + column_child + \"` == t_last_it.child AND assets_clean.name = t_last_it.parent AND lookup_metadata.`\" + column_child + \"` == t_last_it.child AND assets_clean.metadata.treename = '\"+treename+\"'\")\n  df_write.write.insertInto(\"assets_clean\")\n  print(\"--in clean--\")\n            "],"metadata":{},"outputs":[],"execution_count":5}],"metadata":{"name":"Create an asset hierarchy","notebookId":754676554542871},"nbformat":4,"nbformat_minor":0}