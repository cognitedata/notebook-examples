{"cells":[{"cell_type":"markdown","source":["# Exporting data to BigQuery\n\nGoogle provides a [BigQuery connector](https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery) for Hadoop that [can be used with Spark](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example),\nallowing us to easily export data from Cognite Data Fusion to BigQuery.\n\nIn order to do so \"natively\" (without having to call the `bq` command line tool) we'll use Scala instead of Python, and convert our data to the GSON objects required by the BigQuery connector.\nFor an example of using `bq` together with Python, instead of Scala, see the PySpark version of the code in [this tutorial](https://cloud.google.com/dataproc/docs/tutorials/bigquery-connector-spark-example).\n\nImport this notebook to Databricks before continuing. If you are using Jupyter or another notebook environment you will have to make\nsome modifications on your own with regards to setting up credentials and installing the Google Hadoop connectors.\n\nTo run this you need the following:\n- Databricks CLI installed and configured.\n- API key for a Cognite Data Fusion project. You can use a key for [Open Industrial Data](https://openindustrialdata.com/) (the `publicdata` project), saved in a secret in Databricks.\n- Access to a Google Cloud Platform project, with sufficient permissions to create a service account with access to BigQuery, or access to such a service account's key."],"metadata":{}},{"cell_type":"code","source":["%scala\nval secret_scope = \"your_scope\"\nval project = \"publicdata\"\nval resourceType = \"events\"\n\nval gcpProjectId = \"your-project-id\"\nval bigQueryDataset = \"your_bigquery_dataset\"\nval bigQueryTable = \"events\"\nval gcsBucket = \"databricks-bq\"  // used for temporary storage"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Installing Google Hadoop connectors and credentials on the cluster\n\nWe need to install the [Cloud Storage](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage) and the BigQuery Hadoop connectors on our cluster.\nIn addition, all nodes in the cluster need a Google service account credentials file.\n\nOne way to do this is by uploading an init script like this to DBFS, [the Databricks File System](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html).\nFirst you need to create a [Google Service Account and key file](https://cloud.google.com/iam/docs/creating-managing-service-account-keys) with the \"BigQuery Data Owner\" and \"BigQuery Job User\" roles.\n\nThe service account must also be given full permissions to the Google Cloud Storage bucket you specified in `gcsBucket` above.\n\nCopy the contents of the key file into the following init script.\n\n```\n#!/bin/bash\n\nmkdir /google\ncat <<EOF >/google/credentials.json\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"some-gcp-project\",\n  \"private_key_id\": \"2e95aed5e9588518a476111c1111222233334444\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAABC/ABCDEFGHIJK\\nLMNOPQRSTUVWXYZ/ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUV\\nABCDEFGHIJKLMNOPQRSTUVWXYZABCD+3sVnFOW7ggurUCDcpIo13rjA2oZ7aOhNG\\nABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKL\\novFhQkBCERGTvhK3fkGMDdaGJ+vh7yecY59T9F4PivcbGB/5syHYwAiRrb7PzJ/Y\\n/k1/lzjFDdtyyo4qyoit16H6VKROesEmEnBQW8lrrvxfo+UEEcpphF4V4AsACZRa\\nnrbfBgFBAgMBAAECggEAAkrs7mvhpa6b+Z9h4qhDXHGc8QisYmxzjQM9rqD98XKI\\nmTh1eR6Rz+KuPqHjiQTePZ5THWxHEeP7uo9V2nGRitDo/jvqCjjYoUm0ZGsR15gi\\nc1zQk+c4r8dS/uIu7BUWJFEC/wMn3WsM/GzZH99VQHSevcHcMuzYjOnkq2DdEfSQ\\nrqXKRHHDA4ydPMGHuwbOh0ir4i3rwa0cfxv6hkshcV/Y8ef2VTSB34Ga8O8PfHJV\\nd+LRGbDS6poL0pp4Z5m+3OYyWgwW85Wz9jlCpVs7hDUROTrUkClbTEv42wfY12BJ\\nwgZpSWr29Y+C2cUeFZpNVkKmvHyrJSoUeGrd0rQhAQKBgQDPaI20cgK8NMqypM3r\\n3+iCAe3wKk8gSLyAG7OchErqReGIlHtPMaehzllrFfBVS/viq9hCKPv0udIdMA3M\\n/k4tg21ipPw4I6knHQIHNfMmfzUTfjNxnwmLv1qtV83pbCO21x7aG9+0QgG7MDZU\\n5dEYiiA/FPriDVvYThx74KdiIQKBgQC1a52sjfreihdFmx+5LWQQYTaKLH0yqmUK\\ntJX1MbJ4uxB29yUnRsVDJFK0DXxJw+LjN7hBrnbJxKvJChUnM7P5PjS4xVQN3B6T\\nnHBBYAWlGfA2of+Hew7RkbMSj3FpwI/6pA39zL6aIjvlRTkigDLPqGzTBs3WSTUu\\nrtOvPLz7IQKBgEGjENU+D2eIPW1zgkdXQLmD6szKVugcnKreGWU66IpjOxCCDNPv\\nHuGx79JXywrzVO9S+slVNwcnlzrtbjClehAlO4SwObF6d5mNMIsfo6dXMnDMy3L1\\npYu4LvYUh3GLa3H9eiIEGDNvgCTBCTCs2hMuAy5zcUIAgERf09vQKKiBAoGBALLr\\nsZNOa04yroTtWjoMtBUbauiWu9rOBdyrAQHSw4siIjjRwYic2UtAdXgxnJQ7ZnZk\\nq4nLgEe5eRpKoVYKPcLgQKrBogYRQu6daHBxPN/5Vtjqp9J74L96jDsP0PBVAypT\\nxaC63newr5WL0a9e872tA7aTjUl65cJmlUrMAi1BAoGBAJmLlOMjMMoqTzHCX0EB\\nW8p+lYqap05eD7M7M9YXUI5kKx4EZb2hHs9U9QzO2kDDX4sYMsUQs1S8++dhnJ3Z\\nABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVABCDEFGHIJKLMNOP\\nlN8OQNMZJ16xRiIgOMk7eAM2\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"databricks-service-account@some-gcp-project.iam.gserviceaccount.com\",\n  \"client_id\": \"111111111111111111111\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/databricks-service-account%40some-gcp-project.iam.gserviceaccount.com\"\n}\nEOF\n\ncurl https://storage.googleapis.com/hadoop-lib/bigquery/bigquery-connector-hadoop2-latest.jar > /databricks/jars/bigquery-connector-hadoop2-latest.jar\ncurl https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar > /databricks/jars/gcs-connector-hadoop2-latest.jar\n```\n\nSave this using your favorite text editor and use the [Databricks CLI](https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html) to upload it: `dbfs cp my-init-script.sh dbfs:/init_scripts/init_google.sh`\nEdit your cluster, expand the \"Advanced Options\" section and click the \"Init Scripts\" tab. In \"Init Script Path\", enter the location of your init script (for example, `dbfs:/init_scripts/init_google.sh`) and click \"Add\".\n\n**Note that everyone with access to DBFS will be able to read this file.**\nThere's currently no way to get around this, so make sure that it's acceptable that everyone with access to your Databricks\nworkspace can access your service account's credentials.\n\nWe also need to add the following to the Spark config of our cluster (also in the \"Advanced Options\" section):\n\n```\nfs.AbstractFileSystem.gs.impl com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\nfs.gs.impl com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\ngoogle.cloud.auth.service.account.enable true\ngoogle.cloud.auth.service.account.json.keyfile /google/credentials.json\nspark.hadoop.fs.AbstractFileSystem.gs.impl com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\nspark.hadoop.fs.gs.impl com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\nspark.hadoop.google.cloud.auth.service.account.enable true\nspark.hadoop.google.cloud.auth.service.account.json.keyfile /google/credentials.json\n```\n\nThen click \"Confirm and restart\" at the top of the cluster page."],"metadata":{}},{"cell_type":"markdown","source":["## Configure the BigQuery export settings\n\nWe'll set up the configuration to export to our destination table in BigQuery, and configure our GCS bucket to be used as temporary storage.\n\nThe way this works is by having Spark export data in JSON format to a GCS bucket, where the data is then loaded from through a BigQuery import job. The details of the intermediate export to GCS is taken care of by the Hadoop connector.\nIt also supports a \"direct\" mode which does not use GCS, but it's not recommended for loading larger amounts of data.\n\n`Spark -> GCS -> BigQuery`\n\nWe'll configure the bucket to be cleaned up after the export, and use the `WRITE_TRUNCATE` mode for the import to completely replace any existing data.\n\nMake sure a [BigQuery dataset](https://cloud.google.com/bigquery/docs/datasets) with the name you specified in `bigQueryDataset` exists. If not, please create it now before continuing."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport com.google.cloud.hadoop.io.bigquery.BigQueryConfiguration\nimport com.google.cloud.hadoop.io.bigquery.GsonBigQueryInputFormat\nimport com.google.cloud.hadoop.io.bigquery.output.IndirectBigQueryOutputFormat\n\nval conf = spark.sparkContext.hadoopConfiguration\nconf.set(BigQueryConfiguration.PROJECT_ID_KEY, gcpProjectId)\nconf.set(BigQueryConfiguration.GCS_BUCKET_KEY, gcsBucket)\n\nval outputTableId = s\"$gcpProjectId:$bigQueryDataset.$bigQueryTable\"\nval outputGcsPath = s\"gs://$gcsBucket/hadoop/tmp/bigquery/$bigQueryTable\"\n\nconf.set(\"mapreduce.job.outputformat.class\", classOf[IndirectBigQueryOutputFormat[_,_]].getName)\nconf.set(BigQueryConfiguration.OUTPUT_TABLE_WRITE_DISPOSITION_KEY, \"WRITE_TRUNCATE\")\nconf.set(BigQueryConfiguration.OUTPUT_CLEANUP_TEMP_KEY, \"true\")"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["# Create a data frame for the source data\n\nWe use `\"partitions\", \"100\"` to load data in parallel, and since we'll be using this data twice (first for metadata schema inference, then for loading into BigQuery) we use `.cache()`\n\nAs of this writing the default is 20 partitions, but we may have many more cores than that, which is why we increase it."],"metadata":{}},{"cell_type":"code","source":["%scala\nval sourceDf = spark.read.format(\"com.cognite.spark.datasource\")\n  .option(\"apikey\", dbutils.secrets.get(secret_scope, project))\n  .option(\"type\", resourceType)\n  .option(\"partitions\", \"100\")\n  .load()\n  .cache()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Make metadata compatible with BigQuery\n\nBigQuery has [limitations on field names](https://cloud.google.com/bigquery/docs/schemas#column_names) that are not always compatible with what can be found in the metadata field.\nWe'll register a [User Defined Function](https://docs.databricks.com/spark/latest/spark-sql/udf-scala.html) (UDF) to rename the keys to valid BigQuery field names.\n\nIn addition, BigQuery does not allow us to have both a key named `ABC` and `abc`, so we convert all field names to lowercase here."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.functions.to_json\nimport org.apache.spark.sql.functions.col\nimport org.apache.spark.sql.functions.struct\n\nimport org.apache.spark.sql.functions.udf\n\nval bigQueryCompatibleKeys = (m: Map[String, String]) => {\n  if (m != null) {\n    // Fields must contain only letters, numbers, and underscores, start with a letter or underscore, and be at most 128 characters long\n    m.filter(f => f._1 != null).map { case (k, v) =>\n      val newKey = k.replaceAll(\"[^\\\\w_]\", \"_\") match {\n        case x if !x.matches(\"^[a-zA-Z_].*\") => s\"_${x}\"\n        case x => x\n      }\n      (newKey.take(128).toLowerCase, v) }\n  } else {\n    m\n  }\n}\nval bigQueryCompatibleKeysUDF = udf(bigQueryCompatibleKeys)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["## Use schema inference on the metadata field\n\nAfter renaming the metadata fields we'll use Spark's support for schema inference on JSON to get a schema for our metadata.\nBigQuery also supports schema auto-detection, but [it only uses 100 rows](https://cloud.google.com/bigquery/docs/schema-detect#auto-detect) which is not nearly enough for our use case.\n\nUsing Spark we can run schema inference over the full data set."],"metadata":{}},{"cell_type":"code","source":["%scala\nval jsonDf = spark.read.json(\n  sourceDf\n    .where(col(\"metadata\").isNotNull)\n    .withColumn(\"metadata\", bigQueryCompatibleKeysUDF(col(\"metadata\")))\n    .select(to_json(col(\"metadata\"))).as[String]\n)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Convert the inferred schema to a BigQuery table schema\n\nIn order to use the schema with BigQuery we'll need to convert it to a `BigQueryTableSchema`.\nThis is mostly a simple matter of mapping [Spark SQL's data types](https://spark.apache.org/docs/latest/sql-reference.html) to [BigQuery's SQL data types](https://cloud.google.com/bigquery/docs/reference/standard-sql/data-types).\nArrays are converted to BigQuery fields with the field mode set to `REPEATED`.\n\nThe only difficulty is the `metadata` field, where we use the inferred schema from the previous step to create a BigQuery `RECORD` field."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport com.google.cloud.hadoop.io.bigquery.output.BigQueryTableFieldSchema\nimport com.google.cloud.hadoop.io.bigquery.output.BigQueryTableSchema\nimport org.apache.spark.sql.types.DataType\nimport org.apache.spark.sql.types._\nimport collection.JavaConverters._\n\ndef sparkTypeToBigQueryType(c: StructField): BigQueryTableFieldSchema = {\n  val fieldSchema = new BigQueryTableFieldSchema()\n  fieldSchema.setName(c.name)\n  val bqType = c.dataType match {\n    case BinaryType => fieldSchema.setType(\"BYTES\")\n    case BooleanType => fieldSchema.setType(\"BOOL\")\n    //case CalendarIntervalType\n    //case DateType => \"DATE\"\n    case DoubleType => fieldSchema.setType(\"FLOAT64\")\n    case FloatType => fieldSchema.setType(\"FLOAT64\")\n    case IntegerType => fieldSchema.setType(\"INT64\")\n    case LongType => fieldSchema.setType(\"INT64\")\n    //case NullType => \"STRING\"\n    case ShortType => fieldSchema.setType(\"INT64\")\n    case StringType => fieldSchema.setType(\"STRING\")\n    //case TimestampType\n    case ArrayType(t, _) =>\n      val elementType = sparkTypeToBigQueryType(c.copy(dataType = t))\n      fieldSchema.setType(elementType.getType)\n      if (elementType.getType() == \"RECORD\" && elementType.getFields() != null) {\n        fieldSchema.setFields(elementType.getFields())\n      }\n      fieldSchema.setMode(\"REPEATED\")\n    case StructType(fields) =>\n      fieldSchema.setType(\"RECORD\")\n      fieldSchema.setFields(fields.map(sparkTypeToBigQueryType).toList.asJava)\n  }\n  fieldSchema\n}\n\nval bigQuerySchema = new BigQueryTableSchema()\nbigQuerySchema.setFields(sourceDf.schema.map(sf =>\n  if (sf.name == \"metadata\") {\n    val fieldSchema = new BigQueryTableFieldSchema()\n    fieldSchema.setName(sf.name)\n    fieldSchema.setType(\"RECORD\")\n    val fields = jsonDf.schema.map(sf1 => {\n      sparkTypeToBigQueryType(sf1)\n    })\n    fieldSchema.setFields(fields.toList.asJava)\n    fieldSchema\n  } else {\n    sparkTypeToBigQueryType(sf)\n  }\n).toList.asJava)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["## Configure the BigQuery output\n\nThe output will be using the newline delimited JSON format, which is not the most efficient format but which is very well supported.\nWe also use the BigQuery schema we created in the previous step."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport com.google.cloud.hadoop.io.bigquery.BigQueryFileFormat\nimport com.google.cloud.hadoop.io.bigquery.output.BigQueryOutputConfiguration\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat\n\nBigQueryOutputConfiguration.configure(\n    conf,\n    outputTableId,\n    bigQuerySchema,\n    outputGcsPath,\n    BigQueryFileFormat.NEWLINE_DELIMITED_JSON,\n    classOf[TextOutputFormat[_,_]])"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["## Write to BigQuery\n\nThere is one final complication before our data will be accepted by BigQuery. After renaming the keys to lowercase we can end up with multiple keys of the same name. This is technically valid JSON, but BigQuery does not allow it.\n\nWe can get around this by writing the row as JSON using the [Jackson ObjectMapper](https://www.baeldung.com/jackson-object-mapper-tutorial), which removes duplicate keys,\nand then reading it using the [GSON JsonParser](http://tutorials.jenkov.com/java-json/gson-jsonparser.html) to get a GSON JsonObject, which is the format required by the BigQuery Hadoop connector.\n\n**Note that this means duplicated keys will be removed at random**, which can be an issue for certain use cases.\nAn alternative would be to use a more sophisticated way of doing the renaming.\nPlease let us know if you have such a use case, or if you implement such an alternative.\n\nFinally we use `saveAsNewAPIHadoopDataset` with the output configuration from the previous step to get the data into BigQuery."],"metadata":{}},{"cell_type":"code","source":["%scala\nimport com.fasterxml.jackson.databind.ObjectMapper\nimport com.fasterxml.jackson.core.`type`.TypeReference\nimport com.google.gson.JsonParser\n\nimport java.util.HashMap\n\nsourceDf\n  .withColumn(\"metadata\", bigQueryCompatibleKeysUDF(col(\"metadata\")))\n  .select(to_json(struct(sourceDf.schema.map(c => col(c.name)): _*)).alias(\"json\")) // Convert the whole row to JSON.\n  .rdd\n  .mapPartitions { p =>\n    val mapper = new ObjectMapper()\n    val typeRef = new TypeReference[java.util.Map[String, Object]](){}\n    val parser = new JsonParser()\n    p.map { v =>\n      // Remove duplicate keys.\n      val map: HashMap[String, Object] = mapper.readValue(v.getString(0), typeRef)\n      val string = mapper.writeValueAsString(map)\n      // Convert to a GSON JsonObject.\n      // Use null as the Hadoop data set key, which is not used by the BigQuery connector.\n      (null, parser.parse(string).getAsJsonObject())\n    }\n  }\n  .saveAsNewAPIHadoopDataset(conf)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## Use BigQuery\n\nYour data is now available for querying from BigQuery!\n\nHere are some examples you can try using `bq` or the BigQuery UI:\n- Assets:\n```SELECT a.* FROM `your_bigquery_dataset.assets` a\nCROSS JOIN unnest(a.types) AS type\nWHERE type.name = \"valve\"\n``` (lists all assets of type \"valve\"),\n```\nselect id, description from `your_bigquery_dataset.assets`\nwhere exists (\n  select * from unnest(path) as x inner join `your_bigquery_dataset.assets` a on x = a.id where a.description = 'VRD - PH 1STSTGDISCHCLR GAS OUT EQ'\n) order by 1\n``` (descriptions of all assets related to, and including, \"VRD - PH 1STSTGDISCHCLR GAS OUT EQ\")\n- Events: ```\nSELECT count(*), type, subtype FROM `your_bigquery_dataset.events` GROUP BY type, subtype ORDER BY 1 DESC\n``` (number of events by type and subtype)\n- Timeseries metadata: ```\nSELECT metadata.engunits, count(*) FROM `your_bigquery_dataset.timeseries` GROUP BY metadata.engunits ORDER BY 2 DESC\n``` (number of time series by engineering unit)"],"metadata":{}}],"metadata":{"name":"Export to BigQuery","notebookId":1856792696133913},"nbformat":4,"nbformat_minor":0}