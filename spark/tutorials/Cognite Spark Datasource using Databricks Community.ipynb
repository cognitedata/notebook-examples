{"cells":[{"cell_type":"markdown","source":["# Introduction\n\nSee also the [documentation](https://github.com/cognitedata/cdp-spark-datasource#reading-and-writing-cognite-data-platform-resource-types) for examples for each resource type. The general pattern is:\n\n```scala\nmy_data_frame = spark.read.format(\"cognite.spark.v1\") \\\n  .option(\"type\", \"some-resource-type\") \\\n  .option(\"apiKey\", dbutils.secrets.get(\"your-scope\", \"api-key-for-project\"))\n```\nIf you are using the Community version of Databricks, it does not support personal tokens which is used to create secret scope and secrets. Meaning if you are using the Community version of Databricks we need to use the API-keys in the notebook. \n\n\nThe resource types are:\n- `assets`\n- `events`\n- `timeseries` time series metadata\n- `datapoints` data points for a time series, also supports aggregates\n- `raw` \"RAW\" tables, which also require `.option(\"database\", \"some-database\")` and `.option(\"table\", \"some-table\")`\n\nLet's start by reading some data from the `publicdata` project. If you don't have an API key, go get one from the [Open Industrial Data project](https://openindustrialdata.com/)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da266088-2747-4ba4-9bb9-89778cde102d"}}},{"cell_type":"code","source":["#insert API-key to publicdata project\nAPI_KEY=\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e7ba207-aaad-46f9-8385-d77f48b01df7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["assets = spark.read.format(\"cognite.spark.v1\") \\\n    .option(\"type\", \"assets\") \\\n    .option(\"apiKey\", API_KEY) \\\n    .load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26a32b06-d81d-4e9c-b030-e4abe8ee4ac6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# DataFrames\n\nWe get back a [Spark DataFrame](https://spark.apache.org/docs/latest/sql-programming-guide.html) from `spark.read.format...load()`, which \"is conceptually equivalent to a table in a relational database or a data frame in R/Python\".\n\n`spark` is your entry point to the Spark API, and it's a `SparkSession` with a connection to the cluster. You will mostly use it to read data frames, and then interact with the data frames.\n\nYou may have noticed that the command finished almost immediately. The data frame is a lazy data structure, and doesn't actually load any data until it has to. You can view the schema (the column names and their types) by clicking the small arrow next to the output. The schema is constant for assets, so we didn't need to read any data to produce that schema.\n\nData is loaded only when you perform an *action* on a data frame, which requires data to be present. Examples of actions include `.count()` (for counting the rows), `.show()` (for printing the first few rows), `.toPandas()` (for converting to a Pandas data frame, downloading all data to your Python process), and pretty much anything else that uses data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6b976a7-b023-4c28-aafb-9b22b316d9ed"}}},{"cell_type":"code","source":["print(assets.count())\nassets.show()\nassets.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc3fd77d-d6aa-4ce9-88be-3b1e3f45aab9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["DataFrames can be distributed with many partitions being placed on different nodes in our Spark cluster."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"930d9038-9b20-47aa-a936-3c299c7fd2e8"}}},{"cell_type":"code","source":["assets.rdd.getNumPartitions()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c88d8b3-99fd-43e1-a60f-f7fb302fb95a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We can drop and rename columns, getting a DataFrame with a new schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eab1a8e1-da9e-475d-8290-245c0e98ec11"}}},{"cell_type":"code","source":["assets.drop(\"metadata\") \\\n  .drop(\"externalId\") \\\n  .drop(\"source\") \\\n  .withColumnRenamed(\"description\", \"descr\") \\\n  .withColumnRenamed(\"lastUpdatedTime\", \"updatedAt\") \\\n  .printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55051085-503f-426e-938e-004e60559afa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Notebooks have autocompletion built in and you can view keyboard shortcuts by clicking the keyboard icon at the top bar."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"792e8fe6-d2a0-4bc9-8085-401dbfdc4c6f"}}},{"cell_type":"markdown","source":["# Displaying data\n\nIn Databricks there's a convenient `display()` method you can use to show data in data frames (and a few other formats, like pandas and matplotlib figures). Since showing the data requires it to be loaded, this will also trigger an action.\nBy default, only the first 1000 rows are displayed in the widget, even if Spark needs to load more data than this in the background.\n\nNote that you might need to scoll within the widget to show all the results.\n\nYou can sort the rows shown by different columns, and you can expand the \"string to string\" map in the metadata column by clicking the arrow."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb0a66dc-b137-42a3-b561-0e9513e9afb0"}}},{"cell_type":"code","source":["display(assets)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"25d64da2-b27c-4492-b62b-74abd67523c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["# we will explain groupBy() and count() in the section on aggregations\ndisplay(assets.groupBy(assets.parentId).count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd148103-6f72-48ca-b96e-92f24ec923a6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Caching data\n\nAs we mentioned before, the data frame is a lazy structure that loads data when it is needed. Loading data over and over again can be slow and wasteful when we don't absolutely need it to be completely up-to-date.\nIn that case, we can created a cached data frame by adding `.cache()` at the end."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b95abf89-3c7a-4394-a8bb-cd85744c8533"}}},{"cell_type":"code","source":["events = spark.read.format(\"cognite.spark.v1\") \\\n    .option(\"type\", \"assets\") \\\n    .option(\"apiKey\", API_KEY) \\\n    .load() \\\n    .cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ee6588a-fe94-4b57-97b5-7c28cfb6e2e0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["events.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b6c484a-2b48-41d2-bb3f-18cefc9413f0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["print(events.count())\nevents.show()\nevents.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44b6d920-33d0-4852-a39a-5b856fcfac3f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["If we run the same commands again they should finish more quickly (potentially much more quickly if there's a lot of data)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98da16e9-dea1-49b6-ab6f-1958a1b18412"}}},{"cell_type":"code","source":["print(events.count())\nevents.show()\nevents.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07d820ba-4af6-4494-9a72-f19a8648f55d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["All asset data is now kept in memory by Spark, if possible, and reloads will happen only if a node crashes. Even then, only the data that was kept on that node will be reloaded, if possible.\n\nCaching is a good idea if you have a large amount of data that will not be changed.\n\nHowever, if you cache events as above, your cached copy will not receive new events.\nThis might seem obvious, but it means that if you're doing something like waiting for new events that you\nhave just created to show up, you should *not* cache the DataFrame you're using to check for new events!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ffa868b-2b02-41a4-8274-dbcc5dff5308"}}},{"cell_type":"markdown","source":["# Time series metadata\n\nLet's read the time series metadata into another cached data frame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06e335d7-0842-4051-8924-63bd0ea626b1"}}},{"cell_type":"code","source":["tsmd = spark.read.format(\"cognite.spark.v1\") \\\n    .option(\"type\", \"timeseries\") \\\n    .option(\"apiKey\", API_KEY) \\\n    .load() \\\n    .cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ee51525-bdab-47f0-a1d9-97315668fb77"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["tsmd.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d7da90d8-6520-4628-a713-85469cbb9ce9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["tsmd.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"324d6061-f3ca-4f61-ae7f-811ff62424c1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(tsmd)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d125e51-5fa0-4510-970b-12b2368436c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Aggregations\n\nWe can do things like group by and count using PySpark. For example, how many time series do we have per asset?\nOne way to find out is to put time series metadata into different groups based on their asset id, and then count\nthe number of items in each group, and then order the counts in a descending order."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8bb69e3-4074-467e-b825-df0f4f1a755b"}}},{"cell_type":"code","source":["display(tsmd.groupBy(\"assetId\").count().orderBy(\"count\", ascending=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e798197-b9ee-4275-a4eb-03fca0cced1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["How many different asset descriptions do we have, and how many assets per description?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfda6974-75b4-4f4a-90be-0abf35e7efd2"}}},{"cell_type":"code","source":["display(assets.groupBy(\"description\").count().orderBy(\"count\", ascending=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ccc952f-9fdd-489b-821c-f149e3d6e62f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Spark has support for many different [types of aggregations](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData), such as `min`, `max`, `mean`, `sum`, etc.\n\nWe can make a plot of the number of time series associated per asset, to get an overall view of how many time series assets have in general.\n\nSince we have several \"counts\" in this query, we'll use `.withColumn` to rename the first one.\nWe'll say more about `F` in a little bit, but for now we only need to know that `F.col(\"count\")` let's us refer to\nthe column with the name \"count\"."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d76e4927-1b87-4c49-b1c6-d42ddea6aa28"}}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\ndisplay(tsmd.groupBy(\"assetId\").count() \\\n        # We will do two \"count\" calls here, so we need to remember the counts per asset,\n        # by renaming the column named \"count\" at this point to \"countsPerAsset\"\n        .withColumn(\"countsPerAsset\", F.col(\"count\")) \\\n        .groupBy(\"countsPerAsset\") \\\n        # Now we count the number of assets with 1, 2, etc. time series connected to them.\n        .count() \\\n        # In order to avoid a random order in our bar chart we can sort by \"countsPerAsset\"\n        .orderBy(\"countsPerAsset\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"df5ab4a9-4037-44e4-8f9f-ff983d172baa"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["If we just want to know the average number of time series per asset, we can use `agg` and the `avg` function directly."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbf50d0a-c558-4d5b-8147-4f0991d602cf"}}},{"cell_type":"code","source":["display(tsmd.groupBy(\"assetId\").count().agg(F.avg(F.col(\"count\"))))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d5bb4a19-f3ac-4689-a327-6d36b03c24e8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We will see more of `F` from here on, the `pyspark.sql.functions` package.\nImporting it as `F` allows us to use autocompletion to find functions in that package, and avoids\nambiguities for functions like `min`, but it is also common to see individual methods imported like this:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"73c7b761-fcc1-4515-8b99-c05b0f1a2aa5"}}},{"cell_type":"code","source":["from pyspark.sql.functions import avg"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"76bca9b9-249f-4266-842a-bdb67f9e1cdc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Since autocompletion is very useful, we recommend the `F` style."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7fbb762a-ee21-45bf-8947-4b7b5d78fc19"}}},{"cell_type":"markdown","source":["# Filtering\n\nWe can use `.filter` or `.where` (same method by different names) to select a subset of data. `select` can be used to pick out specific columns, or even parts of columns like `metadata.SOURCE_TABLE`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb66c21d-be92-4e4b-85f6-19237baf14d5"}}},{"cell_type":"code","source":["display(assets.where(assets.description == \"VRD - 1ST STAGE COMPRESSOR LUBE OIL HEATER\") \\\n       .select(\"name\", \"description\", \"metadata.SOURCE_TABLE\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8824fe98-55b0-4faa-aff5-a1dcb7e25e9c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Root nodes are defined as having no parent, so their `.parentId` should be null."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c66a5549-3335-42ef-86b5-fe02fc41e55e"}}},{"cell_type":"code","source":["display(assets.where(assets.parentId.isNull()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6613bfa3-2222-409f-9d79-97c16d0aec4a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Similarly, we can look for uncontextualized time series metadata, which have a null `assetId`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4e4e6cc-d3ce-4749-aeed-dbd615431fc5"}}},{"cell_type":"code","source":["display(tsmd.where(tsmd.assetId.isNull()))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"804dd4fe-c2f7-49ca-8bb5-cc845d3ebe91"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["As expected, all time series in `publicdata` are contextualized. We can negate a filter expression using `~`\nto instead filter for time series that have been contextualized.\n\nIn the case of filtering based on non-`NULL` values we can also use `.isNotNull()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"74e2ef65-6c83-413e-a544-c6681aa107be"}}},{"cell_type":"code","source":["print(tsmd.where(~tsmd.assetId.isNull()).count())\nprint(tsmd.where(tsmd.assetId.isNotNull()).count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9961b93c-4223-450a-8b91-fa33e57ee12e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Column objects\n\n`assets.description` and `assets.parentId` return [Column](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.Column) objects.\n\nColumn objects have a wide range of useful methods, and we will see many examples from here on out.\nWe can also construct them from our DataFrame using string indexing, like `assets[\"description\"]`,\nwhich is necessary if the column name contains characters that are not valid Python identifiers.\n\nFor example, we can say `assets[\"VALUE (%C)\"]`. We can also use `F.col(\"VALUE (%C)\")` to create a Column directly.\nHowever, if we do `F.col(\"name\")` and there are several DataFrames involved that have a `name` column, we'd be in trouble\nsince we didn't specify which `name` column we meant, while `assets.name` would have been unambiguous.\n\nWe'll see more of that when looking at joins.\n\nFor those reasons, we recommend indexing the DataFrame (using `.name` when possible) to create Column objects, even if it can become a bit tedious to spell out the DataFrame name.\n\nHowever, in the previous section we use `F.col(\"count\")` because we didn't have a DataFrame object with a column\nnamed count. Our \"count\" column only existed on an intermediate DataFrame. We could have stored that DataFrame and\ngiven it a name, and then we could have used `df.count`, but sometimes it just makes sense to not bother naming each\nintermediate DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0618401-09bb-4032-bb98-550cc5e9f933"}}},{"cell_type":"markdown","source":["# Joins\n\nWe can join data from different data frames together to answer questions like, what are the time series for asset ids `4050790831683279` and `3195126756929465`?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7948cac1-03aa-4d40-891e-3e615447d513"}}},{"cell_type":"code","source":["display(assets.where(assets.id.isin([4050790831683279, 3195126756929465])) \\\n        .join(tsmd, tsmd.assetId == assets.id) \\\n        .select(assets.name, assets.description, tsmd.description, tsmd.name))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99b01a87-2bf7-4012-aad1-7f1b906dbbef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["When doing joins we often have the same column name in both tables, which can cause confusing results. As you can see, we ended up with two `description` columns and two `name` columns.\n\n`.alias` can be used to rename columns and help us keep track of which description belongs to the asset and which one belongs to the time series."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d1df8bda-c682-4d9e-b9c2-a9118428a746"}}},{"cell_type":"code","source":["display(assets.where(assets.id.isin([4050790831683279, 3195126756929465]))\n        .join(tsmd, tsmd.assetId == assets.id)\n        .select(assets.name, assets.description, tsmd.description.alias(\"tsDescription\"), tsmd.name.alias(\"tsName\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acd834c2-6a4f-4bb7-8e51-be7480bc1bd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Data points\n\nWe can retrieve the data for a time series by using the `datapoints` resource type. This one is a bit special, because it will return no data unless you have specified the name(s) of the time series you want to get data for.\n\nAs a consequence, you should *not* cache data frames using the `datapoints` resource type, otherwise the data frame will cache an empty result (and remain empty!) if you don't specify a time series name when querying it."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2794666c-f3c4-48c7-a0d8-931bbe9e7a2d"}}},{"cell_type":"code","source":["dp = spark.read.format(\"cognite.spark.v1\") \\\n    .option(\"type\", \"datapoints\") \\\n    .option(\"apiKey\", API_KEY) \\\n    .load()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ed15e61-4788-4db7-9ced-1ea61a51f6d7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(dp.where(dp.externalId == \"pi:160184\") \\\n        .where(dp.timestamp > F.lit(\"2017-10-01\")) \\\n        .where(dp.timestamp < F.lit(\"2017-10-31\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b37b8522-8937-49ed-a0f4-2a0d66604331"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["If we don't specify an upper bound, [getLatest](https://doc.cognitedata.com/api/0.5/#operation/getLatest) will be\nused to retrieve the maximum timestamp available.\n\nSimilarly, if there is no lower bound the Spark data source will make a query to the time series API to find the timestamp\nof the first available data point."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7eb42dd0-7be5-492c-bfba-6584ca74161a"}}},{"cell_type":"markdown","source":["Raw data points are downloaded by default, but the data points DataFrame also has full support for aggregates."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a021fdd4-0d9a-4c4e-8742-50262c03c8b9"}}},{"cell_type":"code","source":["display(dp.where(dp.externalId == \"pi:160184\") \\\n        .where(dp.granularity == \"7d\") \\\n        .where(dp.aggregation.isin([\"min\", \"average\", \"max\"]))\n        .where(dp.timestamp > F.lit(\"2017-10-01\")) \\\n        .where(dp.timestamp < F.lit(\"2017-10-31\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e7392ed-c342-4f5c-99ea-b8ccc9b34750"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Plotting data\n\nThe `display()` widget has a number of options for showing data in different ways, including a line plot that can group results by a column.\n\nUsing this we can easily create a plot showing the minimum, average, and maximum values for a time series."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"264c5805-4b09-4738-ae88-11c5676856d0"}}},{"cell_type":"code","source":["display(dp.where(dp.externalId == \"pi:160184\") \\\n        .where(dp.granularity == \"1d\") \\\n        .where(dp.aggregation.isin([\"min\", \"average\", \"max\"]))\n        .where(dp.timestamp > F.lit(\"2017-10-01\")) \\\n        .where(dp.timestamp < F.lit(\"2017-10-31\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c78cdf1f-89e9-4f96-9202-6f23253d9372"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Joins with data points\n\nDue to limitations in Spark (that we may perhaps one day be able to work around) it's not possible to join `datapoints` directly, but we can get the externalIds of the time series we want to look at as a Python list by using `.collect()`.\n\nFor example, let's say we want to look at data points from the time series with description `PH 1stStgComp Discharge` that are connected to the assets with description `VRD - PH 1STSTGCOMP DISCHARGE` that we found above. First we get the externalIds of those time series into a Python list."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d9a3d076-db64-4b03-afd7-c6c601d84678"}}},{"cell_type":"code","source":["discharge_time_series = assets.where(assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\") \\\n  .join(tsmd, tsmd.assetId == assets.id) \\\n  .select(tsmd.externalId.alias(\"tsName\"))\ndischarge_time_series_names = [ t.tsName for t in discharge_time_series.collect() ]\ndischarge_time_series_names"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4f370879-33a8-4400-a26a-ddf1db6c2b5d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Then we can use `.where(dp.name.isin(discharge_time_series_names))` to do the join we wanted."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2359b82-ecee-4aa4-ad72-63833df9b5ee"}}},{"cell_type":"code","source":["display(dp.where(dp.externalId.isin(discharge_time_series_names)) \\\n        .where(dp.timestamp > F.lit(\"2017-10-01\")) \\\n        .where(dp.aggregation == 'min') \\\n        .where(dp.granularity == \"7d\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"24b55082-2922-4f83-8b7f-76eaef0bec5d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Files metadata\n\nWe also have support for files metadata. Currently we support reading and updating existing files metadata."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d6e762c0-93c0-4233-9ebc-85a9dd491b38"}}},{"cell_type":"code","source":["files = spark.read.format(\"cognite.spark.v1\") \\\n  .option(\"type\", \"files\") \\\n  .option(\"apiKey\", API_KEY) \\\n  .load() \\\n  .cache()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ec8b31ab-10f6-4b02-adad-57bed999d4b8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["files.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da0a82aa-d6b5-4002-993f-4766466c7c1e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display(files.groupBy(files.mimeType) \\\n        .count() \\\n        .orderBy(\"count\", ascending=False))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5daa44f-cf66-4fd0-b4f3-fc5130c369ca"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4eea9e6c-8e2e-4d2c-9fef-5e56cc0d32b0"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"Tutorial","notebookId":1572025699111510,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"application/vnd.databricks.v1+notebook":{"notebookName":"Cognite Spark Datasource using Databricks Community","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":967342071265178}},"nbformat":4,"nbformat_minor":0}
