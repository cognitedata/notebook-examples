{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "See also the [documentation](https://github.com/cognitedata/cdp-spark-datasource#reading-and-writing-cognite-data-platform-resource-types) for examples for each resource type. The general pattern is:\n",
    "\n",
    "```scala\n",
    "my_data_frame = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "  .option(\"type\", \"some-resource-type\") \\\n",
    "  .option(\"apiKey\", dbutils.secrets.get(\"your-scope\", \"api-key-for-project\"))\n",
    "```\n",
    "\n",
    "The resource types are:\n",
    "- `assets`\n",
    "- `events`\n",
    "- `timeseries` time series metadata\n",
    "- `datapoints` data points for a time series, also supports aggregates\n",
    "- `raw` \"RAW\" tables, which also require `.option(\"database\", \"some-database\")` and `.option(\"table\", \"some-table\")`\n",
    "\n",
    "Let's start by reading some data from the `publicdata` project. If you don't have an API key, go get one from the [Open Industrial Data project](https://openindustrialdata.com/).\n",
    "\n",
    "We'll assume that you have set up a [secret scope](https://docs.databricks.com/user-guide/secrets/index.html) in Databricks with a key `api-key-publicdata` containing your API key for Open Industrial Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_scope = \"your-scope\" # name your secret scope here\n",
    "project_key = \"secret-scope-key\" # name the key to use from secret scope here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6849c9f64b7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.cognite.spark.datasource\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"assets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"apiKey\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecrets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msecret_scope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "assets = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "    .option(\"type\", \"assets\") \\\n",
    "    .option(\"apiKey\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "\n",
    "We get back a [Spark DataFrame](https://spark.apache.org/docs/latest/sql-programming-guide.html) from `spark.read.format...load()`, which \"is conceptually equivalent to a table in a relational database or a data frame in R/Python\".\n",
    "\n",
    "`spark` is your entry point to the Spark API, and it's a `SparkSession` with a connection to the cluster. You will mostly use it to read data frames, and then interact with the data frames.\n",
    "\n",
    "You may have noticed that the command finished almost immediately. The data frame is a lazy data structure, and doesn't actually load any data until it has to. You can view the schema (the column names and their types) by clicking the small arrow next to the output. The schema is constant for assets, so we didn't need to read any data to produce that schema.\n",
    "\n",
    "Data is loaded only when you perform an *action* on a data frame, which requires data to be present. Examples of actions include `.count()` (for counting the rows), `.show()` (for printing the first few rows), `.toPandas()` (for converting to a Pandas data frame, downloading all data to your Python process), and pretty much anything else that uses data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assets.count())\n",
    "assets.show()\n",
    "assets.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames can be distributed with many partitions being placed on different nodes in our Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop and rename columns, getting a DataFrame with a new schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets.drop(\"metadata\") \\\n",
    "  .drop(\"path\") \\\n",
    "  .drop(\"depth\") \\\n",
    "  .withColumnRenamed(\"description\", \"descr\") \\\n",
    "  .withColumnRenamed(\"lastUpdatedTime\", \"updatedAt\") \\\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebooks have autocompletion built in and you can view keyboard shortcuts by clicking the keyboard icon at the top bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying data\n",
    "\n",
    "In Databricks there's a convenient `display()` method you can use to show data in data frames (and a few other formats, like pandas and matplotlib figures). Since showing the data requires it to be loaded, this will also trigger an action.\n",
    "By default, only the first 1000 rows are displayed in the widget, even if Spark needs to load more data than this in the background.\n",
    "\n",
    "Note that you might need to scoll within the widget to show all the results.\n",
    "\n",
    "You can sort the rows shown by different columns, and you can expand the \"string to string\" map in the metadata column by clicking the arrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(assets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will explain groupBy() and count() in the section on aggregations\n",
    "display(assets.groupBy(assets.depth).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caching data\n",
    "\n",
    "As we mentioned before, the data frame is a lazy structure that loads data when it is needed. Loading data over and over again can be slow and wasteful when we don't absolutely need it to be completely up-to-date.\n",
    "In that case, we can created a cached data frame by adding `.cache()` at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "    .option(\"type\", \"assets\") \\\n",
    "    .option(\"apiKey\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
    "    .load() \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events.count())\n",
    "events.show()\n",
    "events.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the same commands again they should finish more quickly (potentially much more quickly if there's a lot of data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(events.count())\n",
    "events.show()\n",
    "events.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All asset data is now kept in memory by Spark, if possible, and reloads will happen only if a node crashes. Even then, only the data that was kept on that node will be reloaded, if possible.\n",
    "\n",
    "Caching is a good idea if you have a large amount of data that will not be changed.\n",
    "\n",
    "However, if you cache events as above, your cached copy will not receive new events.\n",
    "This might seem obvious, but it means that if you're doing something like waiting for new events that you\n",
    "have just created to show up, you should *not* cache the DataFrame you're using to check for new events!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series metadata\n",
    "\n",
    "Let's read the time series metadata into another cached data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsmd = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "    .option(\"type\", \"timeseries\") \\\n",
    "    .option(\"apiKey\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
    "    .load() \\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsmd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsmd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tsmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregations\n",
    "\n",
    "We can do things like group by and count using PySpark. For example, how many time series do we have per asset?\n",
    "One way to find out is to put time series metadata into different groups based on their asset id, and then count\n",
    "the number of items in each group, and then order the counts in a descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tsmd.groupBy(\"assetId\").count().orderBy(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different asset descriptions do we have, and how many assets per description?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(assets.groupBy(\"description\").count().orderBy(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has support for many different [types of aggregations](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData), such as `min`, `max`, `mean`, `sum`, etc.\n",
    "\n",
    "We can make a plot of the number of time series associated per asset, to get an overall view of how many time series assets have in general.\n",
    "\n",
    "Since we have several \"counts\" in this query, we'll use `.withColumn` to rename the first one.\n",
    "We'll say more about `F` in a little bit, but for now we only need to know that `F.col(\"count\")` let's us refer to\n",
    "the column with the name \"count\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "display(tsmd.groupBy(\"assetId\").count() \\\n",
    "        # We will do two \"count\" calls here, so we need to remember the counts per asset,\n",
    "        # by renaming the column named \"count\" at this point to \"countsPerAsset\"\n",
    "        .withColumn(\"countsPerAsset\", F.col(\"count\")) \\\n",
    "        .groupBy(\"countsPerAsset\") \\\n",
    "        # Now we count the number of assets with 1, 2, etc. time series connected to them.\n",
    "        .count() \\\n",
    "        # In order to avoid a random order in our bar chart we can sort by \"countsPerAsset\"\n",
    "        .orderBy(\"countsPerAsset\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to know the average number of time series per asset, we can use `agg` and the `avg` function directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tsmd.groupBy(\"assetId\").count().agg(F.avg(F.col(\"count\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see more of `F` from here on, the `pyspark.sql.functions` package.\n",
    "Importing it as `F` allows us to use autocompletion to find functions in that package, and avoids\n",
    "ambiguities for functions like `min`, but it is also common to see individual methods imported like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since autocompletion is very useful, we recommend the `F` style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering\n",
    "\n",
    "We can use `.filter` or `.where` (same method by different names) to select a subset of data. `select` can be used to pick out specific columns, or even parts of columns like `metadata.SOURCE_TABLE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(assets.where(assets.description == \"VRD - 1ST STAGE COMPRESSOR LUBE OIL HEATER\") \\\n",
    "       .select(\"name\", \"description\", \"metadata.SOURCE_TABLE\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root nodes are defined as having no parent, so their `.parentId` should be null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(assets.where(assets.parentId.isNull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can look for uncontextualized time series metadata, which have a null `assetId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tsmd.where(tsmd.assetId.isNull()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, all time series in `publicdata` are contextualized. We can negate a filter expression using `~`\n",
    "to instead filter for time series that have been contextualized.\n",
    "\n",
    "In the case of filtering based on non-`NULL` values we can also use `.isNotNull()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsmd.where(~tsmd.assetId.isNull()).count())\n",
    "print(tsmd.where(tsmd.assetId.isNotNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Column objects\n",
    "\n",
    "`assets.description` and `assets.parentId` return [Column](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.Column) objects.\n",
    "\n",
    "Column objects have a wide range of useful methods, and we will see many examples from here on out.\n",
    "We can also construct them from our DataFrame using string indexing, like `assets[\"description\"]`,\n",
    "which is necessary if the column name contains characters that are not valid Python identifiers.\n",
    "\n",
    "For example, we can say `assets[\"VALUE (%C)\"]`. We can also use `F.col(\"VALUE (%C)\")` to create a Column directly.\n",
    "However, if we do `F.col(\"name\")` and there are several DataFrames involved that have a `name` column, we'd be in trouble\n",
    "since we didn't specify which `name` column we meant, while `assets.name` would have been unambiguous.\n",
    "\n",
    "We'll see more of that when looking at joins.\n",
    "\n",
    "For those reasons, we recommend indexing the DataFrame (using `.name` when possible) to create Column objects, even if it can become a bit tedious to spell out the DataFrame name.\n",
    "\n",
    "However, in the previous section we use `F.col(\"count\")` because we didn't have a DataFrame object with a column\n",
    "named count. Our \"count\" column only existed on an intermediate DataFrame. We could have stored that DataFrame and\n",
    "given it a name, and then we could have used `df.count`, but sometimes it just makes sense to not bother naming each\n",
    "intermediate DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins\n",
    "\n",
    "We can join data from different data frames together to answer questions like, what are the time series for asset ids `4050790831683279` and `3195126756929465`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(assets.where(assets.id.isin([4050790831683279, 3195126756929465])) \\\n",
    "        .join(tsmd, tsmd.assetId == assets.id) \\\n",
    "        .select(assets.name, assets.description, tsmd.description, tsmd.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing joins we often have the same column name in both tables, which can cause confusing results. As you can see, we ended up with two `description` columns and two `name` columns.\n",
    "\n",
    "`.alias` can be used to rename columns and help us keep track of which description belongs to the asset and which one belongs to the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(assets.where(assets.id.isin([4050790831683279, 3195126756929465]))\n",
    "        .join(tsmd, tsmd.assetId == assets.id)\n",
    "        .select(assets.name, assets.description, tsmd.description.alias(\"tsDescription\"), tsmd.name.alias(\"tsName\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data points\n",
    "\n",
    "We can retrieve the data for a time series by using the `datapoints` resource type. This one is a bit special, because it will return no data unless you have specified the name(s) of the time series you want to get data for.\n",
    "\n",
    "As a consequence, you should *not* cache data frames using the `datapoints` resource type, otherwise the data frame will cache an empty result (and remain empty!) if you don't specify a time series name when querying it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "    .option(\"type\", \"datapoints\") \\\n",
    "    .option(\"apiKey\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dp.where(dp.name == \"VAL_23-FT-92537-01:X.Value\") \\\n",
    "        .where(dp.timestamp > 1507766400000) \\\n",
    "        .where(dp.timestamp < 1517078400000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't specify an upper bound, [getLatest](https://doc.cognitedata.com/api/0.5/#operation/getLatest) will be\n",
    "used to retrieve the maximum timestamp available.\n",
    "\n",
    "Similarly, if there is no lower bound the Spark data source will make a query to the time series API to find the timestamp\n",
    "of the first available data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data points are downloaded by default, but the data points DataFrame also has full support for aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dp.where(dp.name == \"VAL_23-FT-92537-01:X.Value\") \\\n",
    "        .where(dp.granularity == \"7d\") \\\n",
    "        .where(dp.aggregation.isin([\"min\", \"avg\", \"max\"]))\n",
    "        .where(dp.timestamp > 1507766400000) \\\n",
    "        .where(dp.timestamp < 1517078400000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting data\n",
    "\n",
    "The `display()` widget has a number of options for showing data in different ways, including a line plot that can group results by a column.\n",
    "\n",
    "Using this we can easily create a plot showing the minimum, average, and maximum values for a time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dp.where(dp.name == \"VAL_23-FT-92537-01:X.Value\") \\\n",
    "        .where(dp.granularity == \"1d\") \\\n",
    "        .where(dp.aggregation.isin([\"min\", \"avg\", \"max\"])) \\\n",
    "        .where(dp.timestamp > 1507766400000) \\\n",
    "        .where(dp.timestamp < 1517078400000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins with data points\n",
    "\n",
    "Due to limitations in Spark (that we may perhaps one day be able to work around) it's not possible to join `datapoints` directly, but we can get the names of the time series we want to look at as a Python list by using `.collect()`.\n",
    "\n",
    "For example, let's say we want to look at data points from the time series with description `PH 1stStgComp Discharge` that are connected to the assets with description `VRD - PH 1STSTGCOMP DISCHARGE` that we found above. First we get the names of those time series into a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "discharge_time_series = assets.where(assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\") \\\n",
    "  .join(tsmd, tsmd.assetId == assets.id) \\\n",
    "  .select(tsmd.name.alias(\"tsName\"))\n",
    "discharge_time_series_names = [ t.tsName for t in discharge_time_series.collect() ]\n",
    "discharge_time_series_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use `.where(dp.name.isin(discharge_time_series_names))` to do the join we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dp.where(dp.name.isin(discharge_time_series_names)) \\\n",
    "        .where(dp.timestamp > 1507766400000) \\\n",
    "        .where(dp.aggregation == 'min') \\\n",
    "        .where(dp.granularity == \"7d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming data\n",
    "\n",
    "The timestamps we get from the Cognite Data Platform are in [\"Unix time\"](https://en.wikipedia.org/wiki/Unix_time)\n",
    "format with milliseconds, ie. the number of milliseconds since 00:00:00 Thursday, 1 January 1970.\n",
    "\n",
    "That doesn't look so great when we're doing plots, so let's convert them to the timestamp type built into Spark.\n",
    "We can do this using PySpark, and the conversion will be done in parallel across our Spark cluster.\n",
    "\n",
    "Note that Spark *does* support milliseconds in timestamps, but you need to divide the value returned from CDP by 1000.0\n",
    "to get the milliseconds into the fractional part of the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dp.where(dp.name.isin(discharge_time_series_names)) \\\n",
    "        .where(dp.timestamp > 1507766400000) \\\n",
    "        .where(dp.aggregation == \"min\") \\\n",
    "        .where(dp.granularity == \"7d\") \\\n",
    "        .withColumn(\"timestamp\", (dp.timestamp / 1000.0).cast('timestamp'))) # this overrides the previous column named \"timestamp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `display` is still limited to showing only the first 1000 rows, also when doing graphical plots.\n",
    "\n",
    "If you try to make a plot out of a DataFrame with more than 1000 rows, you might get some unexpected results.\n",
    "In the following plot we (may) end up with the first 1000 rows all belonging to the `min` aggregation,\n",
    "and `max` appears to be missing.\n",
    "\n",
    "Choose your aggregates and time ranges wisely! If you see something weird in your plot, first check the number of rows in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dp.where(dp.name.isin(discharge_time_series_names)) \\\n",
    "        .where(dp.timestamp > 1507766400000) \\\n",
    "        .where(dp.aggregation.isin([\"min\", \"max\"])) \\\n",
    "        .where(dp.granularity == \"1h\") \\\n",
    "        .withColumn(\"timestamp\", (dp.timestamp / 1000.0).cast('timestamp')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Files metadata\n",
    "\n",
    "We also have support for files metadata. Currently we support reading and updating existing files metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "  .option(\"type\", \"files\") \\\n",
    "  .option(\"apiKey\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
    "  .load() \\\n",
    "  .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(files.groupBy(files.fileType) \\\n",
    "        .count() \\\n",
    "        .orderBy(\"count\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing data\n",
    "\n",
    "Most resource types also support writing data. Let's copy the assets with description `\"VRD - PH 1STSTGCOMP DISCHARGE\"` to another project.\n",
    "\n",
    "First we need to create a root asset in the destination project to attach them to. In order to do this we'll register a temporary view for our data frame, making it available to SQL.\n",
    "\n",
    "When writing data it's often a good idea to check that it was created. If you try to do that using a cached data frame it won't work (since the cached data isn't reloaded), so you should either not use caching on the data frame you're using, or re-create the data frame to force a reload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_assets = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "    .option(\"type\", \"assets\") \\\n",
    "    .option(\"apiKey\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
    "    .load()\n",
    "dst_assets.createOrReplaceTempView(\"dst_assets\")\n",
    "\n",
    "src_assets = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "    .option(\"type\", \"assets\") \\\n",
    "    .option(\"apiKey\", dbutils.secrets.get(secret_scope, project_key)) \\\n",
    "    .load() \\\n",
    "    .cache()\n",
    "src_assets.createOrReplaceTempView(\"src_assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_assets.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_asset = (None, None, None, \"tutorial root\", None, \"root asset for Spark tutorial\", None, \"spark-tutorial\", None, 0, 0)\n",
    "root_asset_df = spark.createDataFrame([root_asset], schema=dst_assets.schema)\n",
    "root_asset_df.write.insertInto(\"dst_assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dst_assets.where(dst_assets.source == \"spark-tutorial\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remember the id of this new root node in order to attach our assets to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_root_id = dst_assets.where(dst_assets.source == \"spark-tutorial\") \\\n",
    "  .where(dst_assets.parentId.isNull()).collect()[0].id\n",
    "dst_root_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `F.lit` to construct constant literal values and `.withColumn` to change `parentId` to the id of this\n",
    "new root new, and to set `source` to \"spark-tutorial\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_to_copy = src_assets.where(src_assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\") \\\n",
    "  .withColumn(\"parentId\", F.lit(dst_root_id)) \\\n",
    "  .withColumn(\"source\", F.lit(\"spark-tutorial\"))\n",
    "\n",
    "assets_to_copy.write.insertInto(\"dst_assets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dst_assets.where(dst_assets.source == \"spark-tutorial\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now have as many assets with the description `\"VRD - PH 1STSTGCOMP DISCHARGE\"` in the source project\n",
    "as we do in the destination project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src_assets.where(src_assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\").count())\n",
    "print(dst_assets.where(dst_assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL interface\n",
    "\n",
    "We can also write (Spark) SQL to query and insert data, which is quite convenient when doing insert.\n",
    "There's a [large number of functions](https://spark.apache.org/docs/2.4.0/api/sql/index.html) available in this SQL dialect, which is based on [Hive](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select).\n",
    "\n",
    "You can run SQL expressions either using `spark.sql(\"select * from dst_assets limit 5\")` or using `%sql` when starting a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "insert into dst_assets values (null, null, null, 'wjoel-vrd-discharge', null, 'Export of publicdata assets', map(\"Author\", \"wjoel\"), \"spark-tutorial\", null, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(\"select * from dst_assets where metadata.Author = 'wjoel'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from dst_assets where metadata.Author = 'wjoel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select fileType, count(*) from files group by fileType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from tsmd where name like 'VAL_23%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select prefix, count(*)\n",
    "from (select substr(name, 0, 6) as prefix from tsmd)\n",
    "group by prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw\n",
    "\n",
    "You currently need to create a raw database and table using the API directly. We will likely add raw metadata read and write support in the future.\n",
    "\n",
    "If you want to write to a raw table you should set the schema to match the source data before calling `load()`, as in `spark.read ... .schema(sourceDataFrame.schema).load()`.\n",
    "\n",
    "Note that the source data frame *must* include a column named `key` of type string, which will be used as the key for the raw table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognite import CogniteClient\n",
    "\n",
    "client = CogniteClient(api_key=dbutils.secrets.get(\"wjoel\", \"api-key-playground\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"asset_export\"\n",
    "table = \"publicdata\"\n",
    "client.raw.create_databases([db])\n",
    "client.raw.create_tables(database_name=db, table_names=[table])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data frame with a key column, using the asset id as the key. Remember to cast it to a string!\n",
    "\n",
    "assets_for_raw = assets.withColumn(\"key\", assets.id.cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.read.format(\"com.cognite.spark.datasource\") \\\n",
    "    .option(\"type\", \"raw\") \\\n",
    "    .option(\"database\", db) \\\n",
    "    .option(\"table\", table) \\\n",
    "    .option(\"apiKey\", dbutils.secrets.get(\"wjoel\", \"api-key-playground\")) \\\n",
    "    .schema(assets_for_raw.schema) \\\n",
    "    .load()\n",
    "raw_df.createOrReplaceTempView(\"assets_in_raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading data\n",
    "\n",
    "Uploading data can be done using the `dbfs` command line tool, which is part of the Databricks CLI package.\n",
    "\n",
    "`dbfs cp /path/to/my/file/data.csv dbfs:/your-name/your-project/data.csv`\n",
    "\n",
    "It can then be loaded using `spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/your-name/your-project/data.csv\")`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "Tutorial",
  "notebookId": 1428389422928492
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
