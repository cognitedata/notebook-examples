{"cells":[{"cell_type":"markdown","source":["# Introduction\n\nSee also the [documentation](https://github.com/cognitedata/cdp-spark-datasource#reading-and-writing-cognite-data-platform-resource-types) for examples for each resource type. The general pattern is:\n\n```scala\nmy_data_frame = spark.read.format(\"com.cognite.spark.datasource\") \\\n  .option(\"type\", \"some-resource-type\") \\\n  .option(\"apiKey\", dbutils.secrets.get(\"your-name\", \"api-key-for-project\"))\n```\n\nThe resource types are:\n- `assets`\n- `events`\n- `timeseries` time series metadata\n- `datapoints` data points for a time series, also supports aggregates\n- `raw` \"RAW\" tables, which also require `.option(\"database\", \"some-database\")` and `.option(\"table\", \"some-table\")`\n\nLet's start by reading some data from the `publicdata` project. If you don't have an API key, go get one from the [Open Industrial Data project](https://openindustrialdata.com/).\n\nWe'll assume that you have set up a [secret scope](https://docs.databricks.com/user-guide/secrets/index.html) in Databricks with a key `api-key-publicdata` containing your API key for Open Industrial Data."],"metadata":{}},{"cell_type":"code","source":["assets = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"assets\") \\\n    .option(\"apiKey\", dbutils.secrets.get(\"your-scope\", \"api-key-publicdata\")) \\\n    .load()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["# DataFrames\n\nWe get back a [Spark DataFrame](https://spark.apache.org/docs/latest/sql-programming-guide.html) from `spark.read.format...load()`, which \"is conceptually equivalent to a table in a relational database or a data frame in R/Python\".\n\n`spark` is your entry point to the Spark API, and it's a `SparkSession` with a connection to the cluster. You will mostly use it to read data frames, and then interact with the data frames.\n\nYou may have noticed that the command finished almost immediately. The data frame is a lazy data structure, and doesn't actually load any data until it has to. You can view the schema (the column names and their types) by clicking the small arrow next to the output. The schema is constant for assets, so we didn't need to read any data to produce that schema.\n\nData is loaded only when you perform an *action* on a data frame, which requires data to be present. Examples of actions include `.count()` (for counting the rows), `.show()` (for printing the first few rows), `.toPandas()` (for converting to a Pandas data frame, downloading all data to your Python process), and pretty much anything else that uses data."],"metadata":{}},{"cell_type":"code","source":["print(assets.count())\nassets.show()\nassets.toPandas()"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["DataFrames can be distributed with many partitions being placed on different nodes in our Spark cluster."],"metadata":{}},{"cell_type":"code","source":["assets.rdd.getNumPartitions()"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["We can drop and rename columns, getting a DataFrame with a new schema."],"metadata":{}},{"cell_type":"code","source":["assets.drop(\"metadata\") \\\n  .drop(\"path\") \\\n  .drop(\"depth\") \\\n  .withColumnRenamed(\"description\", \"descr\") \\\n  .withColumnRenamed(\"lastUpdatedTime\", \"updatedAt\") \\\n  .printSchema()"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Notebooks have autocompletion built in and you can view keyboard shortcuts by clicking the keyboard icon at the top bar."],"metadata":{}},{"cell_type":"markdown","source":["# Displaying data\n\nIn Databricks there's a convenient `display()` method you can use to show data in data frames (and a few other formats, like pandas and matplotlib figures). Since showing the data requires it to be loaded, this will also trigger an action.\nBy default, only the first 1000 rows are displayed in the widget, even if Spark needs to load more data than this in the background.\n\nNote that you might need to scoll within the widget to show all the results.\n\nYou can sort the rows shown by different columns, and you can expand the \"string to string\" map in the metadata column by clicking the arrow."],"metadata":{}},{"cell_type":"code","source":["display(assets)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# we will explain groupBy() and count() in the section on aggregations\ndisplay(assets.groupBy(assets.depth).count())"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["# Caching data\n\nAs we mentioned before, the data frame is a lazy structure that loads data when it is needed. Loading data over and over again can be slow and wasteful when we don't absolutely need it to be completely up-to-date.\nIn that case, we can created a cached data frame by adding `.cache()` at the end."],"metadata":{}},{"cell_type":"code","source":["events = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"assets\") \\\n    .option(\"apiKey\", dbutils.secrets.get(\"cs-tutorial\", \"api-key-publicdata\")) \\\n    .load() \\\n    .cache()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["events.printSchema()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["print(events.count())\nevents.show()\nevents.toPandas()"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["If we run the same commands again they should finish more quickly (potentially much more quickly if there's a lot of data)."],"metadata":{}},{"cell_type":"code","source":["print(events.count())\nevents.show()\nevents.toPandas()"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["All asset data is now kept in memory by Spark, if possible, and reloads will happen only if a node crashes. Even then, only the data that was kept on that node will be reloaded, if possible.\n\nCaching is a good idea if you have a large amount of data that will not be changed.\n\nHowever, if you cache events as above, your cached copy will not receive new events.\nThis might seem obvious, but it means that if you're doing something like waiting for new events that you\nhave just created to show up, you should *not* cache the DataFrame you're using to check for new events!"],"metadata":{}},{"cell_type":"markdown","source":["# Time series metadata\n\nLet's read the time series metadata into another cached data frame."],"metadata":{}},{"cell_type":"code","source":["tsmd = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"timeseries\") \\\n    .option(\"apiKey\", dbutils.secrets.get(\"cs-tutorial\", \"api-key-publicdata\")) \\\n    .load() \\\n    .cache()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":["tsmd.printSchema()"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["tsmd.count()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["display(tsmd)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["# Aggregations\n\nWe can do things like group by and count using PySpark. For example, how many time series do we have per asset?\nOne way to find out is to put time series metadata into different groups based on their asset id, and then count\nthe number of items in each group, and then order the counts in a descending order."],"metadata":{}},{"cell_type":"code","source":["display(tsmd.groupBy(\"assetId\").count().orderBy(\"count\", ascending=False))"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["How many different asset descriptions do we have, and how many assets per description?"],"metadata":{}},{"cell_type":"code","source":["display(assets.groupBy(\"description\").count().orderBy(\"count\", ascending=False))"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["Spark has support for many different [types of aggregations](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.GroupedData), such as `min`, `max`, `mean`, `sum`, etc.\n\nWe can make a plot of the number of time series associated per asset, to get an overall view of how many time series assets have in general.\n\nSince we have several \"counts\" in this query, we'll use `.withColumn` to rename the first one.\nWe'll say more about `F` in a little bit, but for now we only need to know that `F.col(\"count\")` let's us refer to\nthe column with the name \"count\"."],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as F\n\ndisplay(tsmd.groupBy(\"assetId\").count() \\\n        # We will do two \"count\" calls here, so we need to remember the counts per asset,\n        # by renaming the column named \"count\" at this point to \"countsPerAsset\"\n        .withColumn(\"countsPerAsset\", F.col(\"count\")) \\\n        .groupBy(\"countsPerAsset\") \\\n        # Now we count the number of assets with 1, 2, etc. time series connected to them.\n        .count() \\\n        # In order to avoid a random order in our bar chart we can sort by \"countsPerAsset\"\n        .orderBy(\"countsPerAsset\"))"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["If we just want to know the average number of time series per asset, we can use `agg` and the `avg` function directly."],"metadata":{}},{"cell_type":"code","source":["display(tsmd.groupBy(\"assetId\").count().agg(F.avg(F.col(\"count\"))))"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["We will see more of `F` from here on, the `pyspark.sql.functions` package.\nImporting it as `F` allows us to use autocompletion to find functions in that package, and avoids\nambiguities for functions like `min`, but it is also common to see individual methods imported like this:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import avg"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["Since autocompletion is very useful, we recommend the `F` style."],"metadata":{}},{"cell_type":"markdown","source":["# Filtering\n\nWe can use `.filter` or `.where` (same method by different names) to select a subset of data. `select` can be used to pick out specific columns, or even parts of columns like `metadata.SOURCE_TABLE`."],"metadata":{}},{"cell_type":"code","source":["display(assets.where(assets.description == \"VRD - 1ST STAGE COMPRESSOR LUBE OIL HEATER\") \\\n       .select(\"name\", \"description\", \"metadata.SOURCE_TABLE\"))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["Root nodes are defined as having no parent, so their `.parentId` should be null."],"metadata":{}},{"cell_type":"code","source":["display(assets.where(assets.parentId.isNull()))"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["Similarly, we can look for uncontextualized time series metadata, which have a null `assetId`."],"metadata":{}},{"cell_type":"code","source":["display(tsmd.where(tsmd.assetId.isNull()))"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["As expected, all time series in `publicdata` are contextualized. We can negate a filter expression using `~`\nto instead filter for time series that have been contextualized.\n\nIn the case of filtering based on non-`NULL` values we can also use `.isNotNull()`."],"metadata":{}},{"cell_type":"code","source":["print(tsmd.where(~tsmd.assetId.isNull()).count())\nprint(tsmd.where(tsmd.assetId.isNotNull()).count())"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["# Column objects\n\n`assets.description` and `assets.parentId` return [Column](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.Column) objects.\n\nColumn objects have a wide range of useful methods, and we will see many examples from here on out.\nWe can also construct them from our DataFrame using string indexing, like `assets[\"description\"]`,\nwhich is necessary if the column name contains characters that are not valid Python identifiers.\n\nFor example, we can say `assets[\"VALUE (%C)\"]`. We can also use `F.col(\"VALUE (%C)\")` to create a Column directly.\nHowever, if we do `F.col(\"name\")` and there are several DataFrames involved that have a `name` column, we'd be in trouble\nsince we didn't specify which `name` column we meant, while `assets.name` would have been unambiguous.\n\nWe'll see more of that when looking at joins.\n\nFor those reasons, we recommend indexing the DataFrame (using `.name` when possible) to create Column objects, even if it can become a bit tedious to spell out the DataFrame name.\n\nHowever, in the previous section we use `F.col(\"count\")` because we didn't have a DataFrame object with a column\nnamed count. Our \"count\" column only existed on an intermediate DataFrame. We could have stored that DataFrame and\ngiven it a name, and then we could have used `df.count`, but sometimes it just makes sense to not bother naming each\nintermediate DataFrame."],"metadata":{}},{"cell_type":"markdown","source":["# Joins\n\nWe can join data from different data frames together to answer questions like, what are the time series for asset ids `4050790831683279` and `3195126756929465`?"],"metadata":{}},{"cell_type":"code","source":["display(assets.where(assets.id.isin([4050790831683279, 3195126756929465])) \\\n        .join(tsmd, tsmd.assetId == assets.id) \\\n        .select(assets.name, assets.description, tsmd.description, tsmd.name))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["When doing joins we often have the same column name in both tables, which can cause confusing results. As you can see, we ended up with two `description` columns and two `name` columns.\n\n`.alias` can be used to rename columns and help us keep track of which description belongs to the asset and which one belongs to the time series."],"metadata":{}},{"cell_type":"code","source":["display(assets.where(assets.id.isin([4050790831683279, 3195126756929465]))\n        .join(tsmd, tsmd.assetId == assets.id)\n        .select(assets.name, assets.description, tsmd.description.alias(\"tsDescription\"), tsmd.name.alias(\"tsName\")))"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["# Data points\n\nWe can retrieve the data for a time series by using the `datapoints` resource type. This one is a bit special, because it will return no data unless you have specified the name(s) of the time series you want to get data for.\n\nAs a consequence, you should *not* cache data frames using the `datapoints` resource type, otherwise the data frame will cache an empty result (and remain empty!) if you don't specify a time series name when querying it."],"metadata":{}},{"cell_type":"code","source":["dp = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"datapoints\") \\\n    .option(\"apiKey\", dbutils.secrets.get(\"cs-tutorial\", \"api-key-publicdata\")) \\\n    .load()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["display(dp.where(dp.name == \"VAL_23-FT-92537-01:X.Value\") \\\n        .where(dp.timestamp > 1507766400000) \\\n        .where(dp.timestamp < 1517078400000))"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["If we don't specify an upper bound, [getLatest](https://doc.cognitedata.com/api/0.5/#operation/getLatest) will be\nused to retrieve the maximum timestamp available.\n\nSimilarly, if there is no lower bound the Spark data source will make a query to the time series API to find the timestamp\nof the first available data point."],"metadata":{}},{"cell_type":"markdown","source":["Raw data points are downloaded by default, but the data points DataFrame also has full support for aggregates."],"metadata":{}},{"cell_type":"code","source":["display(dp.where(dp.name == \"VAL_23-FT-92537-01:X.Value\") \\\n        .where(dp.granularity == \"7d\") \\\n        .where(dp.aggregation.isin([\"min\", \"avg\", \"max\"]))\n        .where(dp.timestamp > 1507766400000) \\\n        .where(dp.timestamp < 1517078400000))"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["# Plotting data\n\nThe `display()` widget has a number of options for showing data in different ways, including a line plot that can group results by a column.\n\nUsing this we can easily create a plot showing the minimum, average, and maximum values for a time series."],"metadata":{}},{"cell_type":"code","source":["display(dp.where(dp.name == \"VAL_23-FT-92537-01:X.Value\") \\\n        .where(dp.granularity == \"1d\") \\\n        .where(dp.aggregation.isin([\"min\", \"avg\", \"max\"])) \\\n        .where(dp.timestamp > 1507766400000) \\\n        .where(dp.timestamp < 1517078400000))"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["# Joins with data points\n\nDue to limitations in Spark (that we may perhaps one day be able to work around) it's not possible to join `datapoints` directly, but we can get the names of the time series we want to look at as a Python list by using `.collect()`.\n\nFor example, let's say we want to look at data points from the time series with description `PH 1stStgComp Discharge` that are connected to the assets with description `VRD - PH 1STSTGCOMP DISCHARGE` that we found above. First we get the names of those time series into a Python list."],"metadata":{}},{"cell_type":"code","source":["discharge_time_series = assets.where(assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\") \\\n  .join(tsmd, tsmd.assetId == assets.id) \\\n  .select(tsmd.name.alias(\"tsName\"))\ndischarge_time_series_names = [ t.tsName for t in discharge_time_series.collect() ]\ndischarge_time_series_names"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["Then we can use `.where(dp.name.isin(discharge_time_series_names))` to do the join we wanted."],"metadata":{}},{"cell_type":"code","source":["display(dp.where(dp.name.isin(discharge_time_series_names)) \\\n        .where(dp.timestamp > 1507766400000) \\\n        .where(dp.aggregation == 'min') \\\n        .where(dp.granularity == \"7d\"))"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["# Transforming data\n\nThe timestamps we get from the Cognite Data Platform are in [\"Unix time\"](https://en.wikipedia.org/wiki/Unix_time)\nformat with milliseconds, ie. the number of milliseconds since 00:00:00 Thursday, 1 January 1970.\n\nThat doesn't look so great when we're doing plots, so let's convert them to the timestamp type built into Spark.\nWe can do this using PySpark, and the conversion will be done in parallel across our Spark cluster.\n\nNote that Spark *does* support milliseconds in timestamps, but you need to divide the value returned from CDP by 1000.0\nto get the milliseconds into the fractional part of the value."],"metadata":{}},{"cell_type":"code","source":["display(dp.where(dp.name.isin(discharge_time_series_names)) \\\n        .where(dp.timestamp > 1507766400000) \\\n        .where(dp.aggregation == \"min\") \\\n        .where(dp.granularity == \"7d\") \\\n        .withColumn(\"timestamp\", (dp.timestamp / 1000.0).cast('timestamp'))) # this overrides the previous column named \"timestamp\""],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["Note that `display` is still limited to showing only the first 1000 rows, also when doing graphical plots.\n\nIf you try to make a plot out of a DataFrame with more than 1000 rows, you might get some unexpected results.\nIn the following plot we (may) end up with the first 1000 rows all belonging to the `min` aggregation,\nand `max` appears to be missing.\n\nChoose your aggregates and time ranges wisely! If you see something weird in your plot, first check the number of rows in the DataFrame."],"metadata":{}},{"cell_type":"code","source":["display(dp.where(dp.name.isin(discharge_time_series_names)) \\\n        .where(dp.timestamp > 1507766400000) \\\n        .where(dp.aggregation.isin([\"min\", \"max\"])) \\\n        .where(dp.granularity == \"1h\") \\\n        .withColumn(\"timestamp\", (dp.timestamp / 1000.0).cast('timestamp')))"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["# Files metadata\n\nWe also have support for files metadata. Currently we support reading and updating existing files metadata."],"metadata":{}},{"cell_type":"code","source":["files = spark.read.format(\"com.cognite.spark.datasource\") \\\n  .option(\"type\", \"files\") \\\n  .option(\"apiKey\", dbutils.secrets.get(\"cs-tutorial\", \"api-key-publicdata\")) \\\n  .load() \\\n  .cache()"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"code","source":["files.printSchema()"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["display(files.groupBy(files.fileType) \\\n        .count() \\\n        .orderBy(\"count\", ascending=False))"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["# Writing data\n\nMost resource types also support writing data. Let's copy the assets with description `\"VRD - PH 1STSTGCOMP DISCHARGE\"` to another project.\n\nFirst we need to create a root asset in the destination project to attach them to. In order to do this we'll register a temporary view for our data frame, making it available to SQL.\n\nWhen writing data it's often a good idea to check that it was created. If you try to do that using a cached data frame it won't work (since the cached data isn't reloaded), so you should either not use caching on the data frame you're using, or re-create the data frame to force a reload."],"metadata":{}},{"cell_type":"code","source":["dst_assets = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"assets\") \\\n    .option(\"apiKey\", dbutils.secrets.get(\"cs-tutorial\", \"cs-databricks\")) \\\n    .load()\ndst_assets.createOrReplaceTempView(\"dst_assets\")\n\nsrc_assets = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"assets\") \\\n    .option(\"apiKey\", dbutils.secrets.get(\"cs-tutorial\", \"api-key-publicdata\")) \\\n    .load() \\\n    .cache()\nsrc_assets.createOrReplaceTempView(\"src_assets\")"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"code","source":["dst_assets.printSchema()"],"metadata":{},"outputs":[],"execution_count":71},{"cell_type":"code","source":["root_asset = (None, None, None, \"tutorial root\", None, \"root asset for Spark tutorial\", None, \"spark-tutorial\", None, 0, 0)\nroot_asset_df = spark.createDataFrame([root_asset], schema=dst_assets.schema)\nroot_asset_df.write.insertInto(\"dst_assets\")"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"code","source":["display(dst_assets.where(dst_assets.source == \"spark-tutorial\"))"],"metadata":{},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["We need to remember the id of this new root node in order to attach our assets to it."],"metadata":{}},{"cell_type":"code","source":["dst_root_id = dst_assets.where(dst_assets.source == \"spark-tutorial\") \\\n  .where(dst_assets.parentId.isNull()).collect()[0].id\ndst_root_id"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":["We can use `F.lit` to construct constant literal values and `.withColumn` to change `parentId` to the id of this\nnew root new, and to set `source` to \"spark-tutorial\"."],"metadata":{}},{"cell_type":"code","source":["assets_to_copy = src_assets.where(src_assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\") \\\n  .withColumn(\"parentId\", F.lit(dst_root_id)) \\\n  .withColumn(\"source\", F.lit(\"spark-tutorial\"))\n\nassets_to_copy.write.insertInto(\"dst_assets\")"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"code","source":["display(dst_assets.where(dst_assets.source == \"spark-tutorial\"))"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["We should now have as many assets with the description `\"VRD - PH 1STSTGCOMP DISCHARGE\"` in the source project\nas we do in the destination project."],"metadata":{}},{"cell_type":"code","source":["print(src_assets.where(src_assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\").count())\nprint(dst_assets.where(dst_assets.description == \"VRD - PH 1STSTGCOMP DISCHARGE\").count())"],"metadata":{},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":["# SQL interface\n\nWe can also write (Spark) SQL to query and insert data, which is quite convenient when doing insert.\nThere's a [large number of functions](https://spark.apache.org/docs/2.4.0/api/sql/index.html) available in this SQL dialect, which is based on [Hive](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select).\n\nYou can run SQL expressions either using `spark.sql(\"select * from dst_assets limit 5\")` or using `%sql` when starting a cell."],"metadata":{}},{"cell_type":"code","source":["%sql\ninsert into dst_assets values (null, null, null, 'wjoel-vrd-discharge', null, 'Export of publicdata assets', map(\"Author\", \"wjoel\"), \"spark-tutorial\", null, 0, 0)"],"metadata":{},"outputs":[],"execution_count":82},{"cell_type":"code","source":["display(spark.sql(\"select * from dst_assets where metadata.Author = 'wjoel'\"))"],"metadata":{},"outputs":[],"execution_count":83},{"cell_type":"code","source":["%sql\nselect * from dst_assets where metadata.Author = 'wjoel'"],"metadata":{},"outputs":[],"execution_count":84},{"cell_type":"code","source":["%sql\nselect fileType, count(*) from files group by fileType"],"metadata":{},"outputs":[],"execution_count":85},{"cell_type":"code","source":["%sql\nselect * from tsmd where name like 'VAL_23%'"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"code","source":["%sql\n\nselect prefix, count(*)\nfrom (select substr(name, 0, 6) as prefix from tsmd)\ngroup by prefix"],"metadata":{},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["# Raw\n\nYou currently need to create a raw database and table using the API directly. We will likely add raw metadata read and write support in the future.\n\nIf you want to write to a raw table you should set the schema to match the source data before calling `load()`, as in `spark.read ... .schema(sourceDataFrame.schema).load()`.\n\nNote that the source data frame *must* include a column named `key` of type string, which will be used as the key for the raw table."],"metadata":{}},{"cell_type":"code","source":["from cognite import CogniteClient\n\nclient = CogniteClient(api_key=dbutils.secrets.get(\"wjoel\", \"api-key-playground\"))"],"metadata":{},"outputs":[],"execution_count":89},{"cell_type":"code","source":["db = \"asset_export\"\ntable = \"publicdata\"\nclient.raw.create_databases([db])\nclient.raw.create_tables(database_name=db, table_names=[table])"],"metadata":{},"outputs":[],"execution_count":90},{"cell_type":"code","source":["# Create a data frame with a key column, using the asset id as the key. Remember to cast it to a string!\n\nassets_for_raw = assets.withColumn(\"key\", assets.id.cast(\"string\"))"],"metadata":{},"outputs":[],"execution_count":91},{"cell_type":"code","source":["raw_df = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"type\", \"raw\") \\\n    .option(\"database\", db) \\\n    .option(\"table\", table) \\\n    .option(\"apiKey\", dbutils.secrets.get(\"wjoel\", \"api-key-playground\")) \\\n    .schema(assets_for_raw.schema) \\\n    .load()\nraw_df.createOrReplaceTempView(\"assets_in_raw\")"],"metadata":{},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["# Uploading data\n\nUploading data can be done using the `dbfs` command line tool, which is part of the Databricks CLI package.\n\n`dbfs cp /path/to/my/file/data.csv dbfs:/your-name/your-project/data.csv`\n\nIt can then be loaded using `spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/your-name/your-project/data.csv\")`"],"metadata":{}}],"metadata":{"name":"Tutorial","notebookId":1428389422928492},"nbformat":4,"nbformat_minor":0}
