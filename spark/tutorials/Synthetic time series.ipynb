{"cells":[{"cell_type":"markdown","source":["# Synthetic time series\n\nWhen creating synthetic time series we often want to perform calculations on mutiple time series per timestamp.\nIn this notebook you'll find an example of how to do so. Feel free to copy this notebook and use it as template for your own sythetic time series.\n\n## Create your DataFrame to access your data points in clean\n\nSince you want to read your data points in the Cognite Data Platform, you need to create a Spark DataFrame for them using `.option(\"type\", \"datapoints\")` with Cognite's Spark data source."],"metadata":{}},{"cell_type":"code","source":["dp = spark.read.format(\"com.cognite.spark.datasource\") \\\n    .option(\"apiKey\", dbutils.secrets.get(\"linda\", \"publicdata\")) \\\n    .option(\"type\", \"datapoints\") \\\n    .load()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":["## Pivot the data points of the time series you need\n\nNatively we look at the data points in CDP in a narrow and tall format like this:\n\n| name          | timestamp | value |\n|---------------|-----------|-------|\n| time series 1 | 1         | 5     |\n| time series 1 | 2         | 6     |\n| time series 1 | 3         | 7     |\n| time series 2 | 1         | 8     |\n| time series 2 | 2         | 9     |\n| time series 2 | 3         | 10    |\n\nIn order to do row based calculations, we need to get them into a wide format like that:\n\n| timestamp | time series 1 | time series 2 |\n|-----------|---------------|---------------|\n| 1         | 5             | 8             |\n| 2         | 6             | 9             |\n| 3         | 7             | 10            |\n\nSo we select the `name`, `timestamp` and `value` of the time series of interest: `dp.select(\"value\", \"timestamp\", \"name\").where(dp.name.isin([\"VAL_23-ESDV-92501A-PST:VALUE\",\"VAL_23-ESDV-92501B-PST:VALUE\"]))`, with `VAL_23-ESDV-92501A-PST:VALUE` and `VAL_23-ESDV-92501B-PST:VALUE` being the `name`s of our time series.\n\nThen we need to make sure that all our time series have values at the same time stamps, by using CDP's aggregates. `.where(dp.aggregation.isin('avg')).filter(dp.granularity == '5m')\\`.  **Keep in mind that CDP aggregates are \"best effort\" and might not be available or updated for all time windows if you recently ingested or changed data**\n- In this example we use a granularity of 5 minutes. That also means that the synthetic time series we create will have values every 5 minutes. You can adapt this to fit your needs.  `.filter(dp.granularity == '5m')\\` *TODO: docu on possible granularity*\n- Here we use the avarage values of our time series in the chosen granularity. The aggregation you want to use here depends on your data. You can adapt this to fit your needs.  `.where(dp.aggregation.isin('avg'))` *TODO: docu on possible aggregation functions*\n\nWe will group by timestamp (`.groupBy(\"timestamp\")`), to get all values of all time series in one row that were measured at the same time into one group.\n\nTo get to a wide table, we will pivot on the `name` of the time series (`.pivot(\"name\")`), so each unique name value will become columns. When we use the pivot function, we need to specify an aggregation. That is to tell Spark what to do if two values end up in the same group. Pivoting by name and grouping by timestamp on CDP data points will never result in that situation, but we still need to specify it. In thise case we chose `.sum(\"value\")`."],"metadata":{}},{"cell_type":"code","source":["pivot = dp.select(\"value\", \"timestamp\", \"name\").where(dp.name.isin([\"VAL_23-FT-92537-01:X.Value\",\"VAL_23_FT_92537_03:Z.X.Value\"])).where(dp.aggregation.isin('avg')).filter(dp.granularity == '5m')\\\n  .groupBy(\"timestamp\")\\\n  .pivot(\"name\")\\\n  .sum(\"value\")"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["##  Apply calculations\n\nWe'll import everything from `pyspark.sql.functions` to be able to use `lit` (to define Spark SQL constants), `col` (to refer to columns using strings), `when` (similar to a SQL CASE statement), etc."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["We'll refer to the columns of the time series we were looking at above in our calculations several times, so we can shorten the calculations by declaring some variables."],"metadata":{}},{"cell_type":"code","source":["c1 = col(\"`VAL_23-FT-92537-01:X.Value`\")\nc2 = col(\"`VAL_23_FT_92537_03:Z.X.Value`\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["Now we can easily create a DataFrame to hold the results for our synthetic time series.\n\n- We'll name our new DataFrame `synth_dp` with a `name` column, which should have the name of our new time series as its value. We'll name the new time series `name of your new ts` and use `lit` to create this constant value: `synth_dp = pivot.withColumn(\"name\", lit(\"name of your new ts\"))`\n- Then we create a new column named `value` and fill it with the output of our calculations. To define what calcuation needs to be done, we use [pyspark's column functions](http://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html#pyspark.sql.Column): `.withColumn(\"value\", when(c1.isNull(), -2.0).when(c1 > 6224.0, c2*2.0 / c1).otherwise(c1*c2))`\n- When writing data points to CDP we need to satisfy CDP Spark data source's data points schema which contains `aggregation` and `granularity`. When writing data we always write raw data points, so we should set those columns to `None`: `.withColumn(\"aggregation\",lit(None)).withColumn(\"granularity\", lit(None))`\n- Finally we select all the columns we need for the data points schema: `.select(\"name\", \"timestamp\", \"value\", \"aggregation\", \"granularity\")`"],"metadata":{}},{"cell_type":"code","source":["synth_dp = pivot.withColumn(\"name\", lit(\"name of your new ts\"))\\\n  .withColumn(\"value\", when(c1.isNull(), -2.0).when(c1 > 6224.0, c2*2.0 / c1).otherwise(c1*c2))\\\n  .withColumn(\"aggregation\", lit(None))\\\n  .withColumn(\"granularity\", lit(None))\\\n  .select(\"name\", \"timestamp\", \"value\", \"aggregation\", \"granularity\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["display(synth_dp)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["##  Write back to CDP\n\nNow that we have a proper data frame with our brand new synthetic timeseries, we want to store it back.\nFor that we need a DataFrame referencing at a data points resource with an API key that has write permissions (which we don't have for Open Industrial Data).\n\nRequirements to run this sucessfully is that the metadata for your target time series has already been created. In other words, you need to have a time series named `name of your new ts` in your destination CDP project."],"metadata":{}},{"cell_type":"code","source":["dp.createOrReplaceTempView(\"dp\")\nsynth_dp.write.insertInto(\"dp\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["##  Congratulations, you're done!"],"metadata":{}},{"cell_type":"markdown","source":["***But wait. What if my calculations are super complicated and cannot be done with the column functions?***\n\n## Create a UDF for your calculation\n\nA User Defined Function (UDF) is the most powerful option in Spark (but also the least performant one). It can do almost anything you can do using normal Python code, and you can use Python libraries in it as you please.\n\nWhen using `Column` and `pyspark.sql.functions` all the transformations will be executed at full speed as Java byte code, and error messages might be more helpful as well. However, if you really cannot do your super complicated calculation using `Column` and `pyspark.sql.functions`, [UDFs](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html) might be the way to go.\n\nLet's create a normal Python function and put it in a UDF."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\nfrom pyspark.sql.functions import udf\n\ndef randomCalculation_(value1, value2):\n  if value1 == None:\n    return -2\n  elif value1 > 6224:\n    return ((value2*2)/value1)\n  else:\n    return value1*value2\n\nrandomCalculation = udf(lambda x, y: randomCalculation_(x, y), DoubleType())"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Creating your dataframe using that UDF then looks like this:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nc1 = col(\"`VAL_23-FT-92537-01:X.Value`\")\nc2 = col(\"`VAL_23_FT_92537_03:Z.X.Value`\")\n\nsynth_dp = pivot.withColumn(\"name\", lit(\"name of your new ts\"))\\\n  .withColumn(\"value\", randomCalculation(c1,c2))\\\n  .withColumn(\"aggregation\", lit(None))\\\n  .withColumn(\"granularity\", lit(None))\\\n  .select(\"name\", \"timestamp\", \"value\", \"aggregation\", \"granularity\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Alternatively you can use [Pandas UDFs](https://docs.databricks.com/spark/latest/spark-sql/udf-python-pandas.html), which will give you better performance.\n\nInstead of calling your UDF for each row, Pandas UDFs are called with `pandas.Series` arguments containing the values for multiple rows of a column, and should also return a `pandas.Series` with one value for each row.\n\nSimilar to `Column` in PySpark, `pandas.Series` has support for builtins like `>`, `*`, `/` etc. so in this case our code looks much the same, but will run much faster."],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\n\nfrom pyspark.sql.functions import col, pandas_udf\nfrom pyspark.sql.types import LongType\n\n# Declare the function and create the UDF\ndef randomCalculationPandas(value1, value2):\n  if value1 == None:\n    return -2\n  elif value1 > 6224:\n    return ((value2*2)/value1)\n  else:\n    return value1*value2\n\nrandomCalculationPandas = pandas_udf(randomCalculationPandas, returnType=DoubleType())"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#### Creating your DataFrame using that Pandas UDF then looks like this:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\nc1 = col(\"`VAL_23-FT-92537-01:X.Value`\")\nc2 = col(\"`VAL_23_FT_92537_03:Z.X.Value`\")\n\n\nsynth_dp = pivot.withColumn(\"name\", lit(\"name of your new ts\"))\\\n  .withColumn(\"value\", randomCalculationPandas(c1,c2))\\\n  .withColumn(\"aggregation\", lit(None))\\\n  .withColumn(\"granularity\", lit(None))\\\n  .select(\"name\", \"timestamp\", \"value\", \"aggregation\", \"granularity\")"],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"Synthetic time series","notebookId":4149517249124899},"nbformat":4,"nbformat_minor":0}